# -*- coding: utf-8 -*-
"""DigitalImageProcessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vJEMUf1oZkWqNrQ1hJygvULdNtbNfxOp

Load the dataset
"""

from tensorflow.keras.datasets import mnist

#Loads the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

#Display the number of samples in the x_train, x_test, y_train and y_test
print("Initial shape of dimensions of x_train", str(x_train.shape) + "\n")
print("Number of samples in our training data: " + str(len(x_train)))
print("Number of labels in our training data: " + str(len(y_train)))
print("Number of samples in our test data: " + str(len(x_test)))
print("Number of labels in our test data: " + str(len(y_test)) + "\n")

print("Dimensions of the x_train: " + str(x_train[0].shape))
print("Labels in y_train: " + str(y_train.shape))

print("Dimensions of the x_train: " + str(x_test[0].shape))
print("Labels in y_train: " + str(y_test.shape))

"""Take a look at the images"""

import matplotlib.pyplot as plt
import numpy as np

#Plot 6 images in subplots
#set the colormap to grey since our image data is in greyscale
plt.subplot(331)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

plt.subplot(332)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

plt.subplot(333)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

plt.subplot(334)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

plt.subplot(335)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

plt.subplot(336)
random_num = np.random.randint(0, len(x_train))
_=plt.imshow(x_train[random_num],cmap=plt.get_cmap('gray'))

#store the number of rows and columns
img_rows = x_train[0].shape[0]
img_columns = x_train[0].shape[1]

#get the data into the right shape for keras
#add 4th dimension to the tensor
x_train = x_train.reshape(x_train.shape[0], img_rows, img_columns, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_columns, 1)

#store the shape of a single image for future use
input_shape= (img_rows, img_columns, 1)

#change the image type to float32 from unsignedint
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

#normalzie the data by changing the range from 0-255 to 0-1
x_train /=255.0
x_test /=255.0

print("X_train shape ", x_train.shape)
print(x_train.shape[0], "x_train samples")
print(x_test.shape[0], "x_test samples")

from tensorflow.keras.utils import to_categorical
#one-hot encode for outputs
y_train =to_categorical(y_train)
y_test = to_categorical(y_test)

#count the number of columns in our hot coded matrix
print('Number of classes: '+str(y_test.shape[1]))
num_classes=y_test.shape[1]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import SGD

#create a model
model=Sequential()

#First Convolutional layer, filter size 32, which reduces our layer size to 28*28*32
#we use ReLU activation and specify our input shape which is 28*28*1
model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=input_shape))

#second Convolutional layer, filter size 64, which reduces layer size to 24*24*64
model.add(Conv2D(64, (3,3),activation='relu'))

#we use maxpooling with kernel size of 2*2, which reduces size to 12*12*64
model.add(MaxPooling2D(pool_size=(2,2)))

#Dropout P setting as 0.25 to reduce overfitting
model.add(Dropout(0.25))

#We then flatten our tensor object before input into our dense layer
#A flatten operation on a tensor reshapes the tensor to have a shape that is
#equal to the number of elements combined in tensor
#in our CNN it goes from  12*12*64 to 9216*1
model.add(Flatten())

#We use another dropout layer
model.add(Dropout(0.5))

#We create fully connected/Dense layer with an output of each class (10)
model.add(Dense(num_classes, activation='softmax'))

#we compile our model, this creates an object that stores the model. We set the 
#Optimizer to use stochastic Gradient Descent (learning rate of 0.01)
#We set the loss function to be categorical_crossentropy as its suitable for
#multiclass problems. And finally the  metrics (to judge the performance of the model) we use accuracy
model.compile(loss='categorical_crossentropy', optimizer=SGD(0.01), metrics=['accuracy'])

#the summary function can be used to display the model layers and parameters
print(model.summary)

batch_size=32
epochs=10

#store the results for later plotting
#in our fit function we specify  our dataset (x_train and y_train)
#batch size (typically 16 to 128 -- RAM). The number of epochs (10 to 100)
#validation dataset (x_test, y_test)
#verbose = 1, setting the  training to output performance metrics every epoch

history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))

#We then obtain the accuracy score
score =model.evaluate(x_test, y_test, verbose=0)
print("test loss: ", score[0])
print("Test accuracy", score[1])

#Plotting the loss charts
#Use the history object to get our svaed performace results
history_dict=history.history

#extract the loss and the validation losses
loss_values=history_dict['loss']
val_loss_values=history_dict['val_loss']

#get the number of epochs and create an array up to that number using range()
epochs=range(1, len(loss_values) +1)

#Plot line charts for both validation and loss
line1 = plt.plot(epochs, val_loss_values, label='Validation/Test loss')
line2 = plt.plot(epochs, loss_values, label='Training loss')
plt.setp(line1, linewidth=2.0, marker='+', markersize=10.0)
plt.setp(line2, linewidth=2.0, marker='4', markersize=10.0)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.show()

"""Plot of the Accuracy"""

#Plotting the accuracy chart
import matplotlib.pyplot as plt

#Use the history object to get our svaed performace results
history_dict=history.history

#extract the loss and the validation losses
acc_values=history_dict['accuracy']
val_acc_values=history_dict['val_accuracy']

#get the number of epochs and create an array up to that number using range()
epochs=range(1, len(acc_values) +1)

#Plot line charts for both validation and loss
line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')
line2 = plt.plot(epochs, acc_values, label='Training')
plt.setp(line1, linewidth=2.0, marker='+', markersize=10.0)
plt.setp(line2, linewidth=2.0, marker='4', markersize=10.0)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()
plt.show()

#saving the model
model.save('mnist_simple_cnn_10_Epochs.h5')
print('model is saved')

from tensorflow.keras.models import load_model
classifier=load_model('mnist_simple_cnn_10_Epochs.h5')

import numpy as np
figure=plt.figure(figsize=(20,20))
for i in range(5):
  figure.add_subplot(1,5,i+1)
  random_idx=np.random.randint(0,len(x_test))
  plt.imshow(x_test[random_idx,:,:,0],cmap='gray')
  plt.axis('off')
  print(np.squeeze(np.argmax(model.predict(x_test[random_idx].reshape(1,28,28,1)), axis=1),axis=0))

"""In depth visualitazion of Convolutional Neural Networks"""